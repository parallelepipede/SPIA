{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install annoy\n",
    "!pip install apache_beam\n",
    "!pip install tensorflow_hub\n",
    "!pip install --upgrade --force-reinstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e02137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "import sklearn\n",
    "\n",
    "tf.__version__\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_fn = None\n",
    "model_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "\n",
    "def generate_embeddings(text, model_url, random_projection_matrix=None):\n",
    "    # Beam will run this function in different processes that need to\n",
    "    global embed_fn\n",
    "    if embed_fn is None:\n",
    "        embed_fn = hub.load(model_url)\n",
    "        embedding = embed_fn(text).numpy()\n",
    "    if random_projection_matrix is not None:\n",
    "        embedding = random_projection_matrix.fit_transform(embedding)\n",
    "        print(embedding.size)\n",
    "        #embedding = embedding.dot(random_projection_matrix)\n",
    "    return text, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39109336",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 'lowercase_words.txt'\n",
    "reduced_voc = 'lowercase_reduced_words.txt'\n",
    "extension = ['es','s','d','ed','ment', 'ement','ive','ing','ion','ions','ted',\n",
    "             'red','ded','ence','rence', 'ly', 'y']\n",
    "def preprocess_vocabulary() : \n",
    "    def is_extension(root, word):\n",
    "        for ext in extension : \n",
    "            if root + ext == word : \n",
    "                return True\n",
    "            if root[:-1] + ext == word : \n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    with open(vocabulary, 'r') as voc : \n",
    "        lines = voc.readlines()\n",
    "        lines = list(map(str.strip,lines))\n",
    "\n",
    "    with open(reduced_voc, 'w') as voc : \n",
    "        index = 0\n",
    "        while index < len(lines) :\n",
    "            j = index + 1\n",
    "            while j < len(lines) and is_extension(lines[index], lines[j]): \n",
    "                j += 1\n",
    "            voc.write(lines[index]+'\\n')\n",
    "            index = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(reduced_voc, 'r') as voc : \n",
    "    lines = voc.readlines()\n",
    "    print(\"Number of lines for\")\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_projection_matrix(projected_dim):\n",
    "    return GaussianRandomProjection(n_components=projected_dim)\n",
    "\n",
    "\n",
    "def generate_embeddings(text, model_url, random_projection_matrix=None):\n",
    "    # Beam will run this function in different processes that need to\n",
    "    global embed_fn\n",
    "    if embed_fn is None:\n",
    "        embed_fn = hub.load(model_url)\n",
    "    embedding = embed_fn(text).numpy()\n",
    "    if random_projection_matrix is not None:\n",
    "        print(random_projection_matrix)\n",
    "        embedding = random_projection_matrix.fit_transform(embedding)\n",
    "        print(\"n_components : \",random_projection_matrix.n_components, \"n_features_in : \", random_projection_matrix.n_features_in_)\n",
    "        print('Storing random projection matrix to disk...')\n",
    "        with open('random_projection_matrix2', 'wb') as handle:\n",
    "            pickle.dump(random_projection_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fadcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_dim = 64\n",
    "original_dim = hub.load(model_url)(['']).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee10f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = tempfile.mkdtemp()\n",
    "original_dim = hub.load(model_url)(['']).shape[1]\n",
    "random_projection_matrix = None\n",
    "\n",
    "if projected_dim:\n",
    "    random_projection_matrix = generate_random_projection_matrix(projected_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(reduced_voc, index_filename, vector_length, metric='angular', num_trees=100):\n",
    "    \n",
    "    '''Builds an ANNOY index'''\n",
    "    annoy_index = annoy.AnnoyIndex(vector_length, metric = metric)\n",
    "    # Mapping between the item and its identifier in the index\n",
    "    mapping = {}\n",
    "\n",
    "    with open(reduced_voc, 'r') as reduced_voc : \n",
    "        words_list = reduced_voc.readlines()\n",
    "    words_list = list(map(str.strip,words_list))\n",
    "    num_words = len(words_list)\n",
    "\n",
    "    item_counter = 0\n",
    "    embeddings = generate_embeddings(words_list,model_url,random_projection_matrix)\n",
    "\n",
    "    for i, embed in enumerate(embeddings):\n",
    "        mapping[i] = words_list[i]\n",
    "        annoy_index.add_item(i,embed)\n",
    "        if i % 10_000 == 0 : \n",
    "            print(f'{i} items loaded to the index')\n",
    "\n",
    "    print('Building the index with {} trees...'.format(num_trees))\n",
    "    annoy_index.build(n_trees=num_trees)\n",
    "    print('Index is successfully built.')\n",
    "\n",
    "    print('Saving index to disk...')\n",
    "    annoy_index.save(index_filename)\n",
    "    print('Index is saved to disk.')\n",
    "    print(\"Index file size: {} GB\".format(\n",
    "        round(os.path.getsize(index_filename) / float(1024 ** 3), 5)))\n",
    "    annoy_index.unload()\n",
    "\n",
    "    print('Saving mapping to disk...')\n",
    "    with open(index_filename + '.mapping', 'wb') as handle:\n",
    "        pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Mapping is saved to disk.')\n",
    "    print(\"Mapping file size: {} MB\".format(\n",
    "        round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_filename = \"index2\"\n",
    "\n",
    "!rm {index_filename}\n",
    "!rm {index_filename}.mapping\n",
    "\n",
    "%time build_index(reduced_voc, index_filename, projected_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
